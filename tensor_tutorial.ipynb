{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tensors\n",
    "--------------------------------------------\n",
    "Tensors 는 arrays, matrices와 매우 유사한 특화된 자료 구조이다.      \n",
    "PyTorch에서, model의 inputs, outputs, parameters를 인코딩하기 위해 tensor를 사용한다.  \n",
    "<br/>\n",
    "Tensors 는 NumPy의 `ndarray`와 유사하지만, GPUs 또는 기타 특수 하드웨어에서 컴퓨팅 가속화를 위해 동작한다는 점에서 `ndarray`와 다르다.  \n",
    "만약 `ndarray`에 익숙하다면, 좀 더 편하게 Tensor API를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Initialization\n",
    "~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Tensors는 다양한 방법을 사용해 초기화된다. 다음의 예제를 한번 보자:\n",
    "\n",
    "**Directly from data**\n",
    "\n",
    "Tensors는 data에서 바로 만들어지고, data type은 자동으로 지정된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data : \n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(\"x_data : \\n{0}\\n\".format(x_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From a NumPy array**\n",
    "\n",
    "Tensors는 NumPy arrays를 사용해서 만들 수 있다. (and vice versa - see `bridge-to-np-label`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From another tensor:**\n",
    "\n",
    "새로운 tensor는 명시적으로 override되지 않는 한, argument tensor의 속성(shape, datatype)을 유지한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ones : \n",
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "\n",
      "x_rand :\n",
      "tensor([[0.6931, 0.6215],\n",
      "        [0.2459, 0.0442]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # x_data의 속성을 유지한다.\n",
    "print(\"x_ones : \\n{0}\\n\".format(x_ones))\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # x_data의 datatype을 override했다.\n",
    "print(\"x_rand :\\n{0}\\n\".format(x_rand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With random or constant values:**\n",
    "\n",
    "``shape``은 tensor의 차원을 나타내고, datatype은 tuple이다.  \n",
    "아래의 함수에서, ``shape``이 output tensor의 차원을 나타내는 것을 볼 수 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      "tensor([[0.0046, 0.4561, 0.1619],\n",
      "        [0.9186, 0.7194, 0.3573]])\n",
      "\n",
      "Ones Tensor: \n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "Zeros Tensor: \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(\"Random Tensor: \\n{0}\\n\".format(rand_tensor))\n",
    "print(\"Ones Tensor: \\n{0}\\n\".format(ones_tensor))\n",
    "print(\"Zeros Tensor: \\n{0}\\n\".format(zeros_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Attributes\n",
    "~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Tensor attributes는 tensor의 shape, datatype, 그리고 tensor가 저장되는 device(CPU or GPU)를 나타낸다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(\"Shape of tensor: {0}\".format(tensor.shape))\n",
    "print(\"Datatype of tensor: {0}\".format(tensor.dtype))\n",
    "print(\"Device tensor is stored on: {0}\".format(tensor.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Operations\n",
    "~~~~~~~~~~~~~~~~~\n",
    "\n",
    "100개 이상의 tensor 연산이 존재한다.\n",
    "`here <https://pytorch.org/docs/stable/torch.html>`  \n",
    "\n",
    "각 연산들은 GPU에서 실행될 수 있다. (CPU보다 속도가 빠르다)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU를 사용할 수 있는 환경이라면, tensor를 GPU로 옮길 수 있다.\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "list에서 사용하는 몇 가지 연산을 수행해보자.  \n",
    "만약 NumPy API에 익숙하다면, Tensor API를 사용하기 쉬울 것이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard numpy-like indexing and slicing:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0 # 1번 column을 0으로 바꾸기\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joining tensors**  \n",
    "<br/>\n",
    "``torch.cat``로 tensor들을 연결하는 concatenate 연산을 수행할 수 있다.  \n",
    "<br/>\n",
    "`torch.stack <https://pytorch.org/docs/stable/generated/torch.stack.html>` 역시 tensor들을 연결하기 위해 사용할 수 있다. ``torch.cat``과는 미묘하게 다르다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiplying tensors**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) : \n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "\n",
      "tensor * tensor : \n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# element-wist product\n",
    "print(\"tensor.mul(tensor) : \\n{0}\\n\".format(tensor.mul(tensor)))\n",
    "# 또 다른 방법:\n",
    "print(\"tensor * tensor : \\n{0}\\n\".format(tensor * tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "두 개의 tensor들로 matrix multiplication을 계산한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.matmul(tensor.T) \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"tensor.matmul(tensor.T) \\n {0} \\n\".format(tensor.matmul(tensor.T)))\n",
    "# 또 다른 방법:\n",
    "print(\"tensor @ tensor.T \\n {0} \\n\".format(tensor @ tensor.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In-place operations**\n",
    "<br/>\n",
    "메소드 뒤에 ``_``를 붙이면 계산 결과를 반환하고, 그 값을 해당 변수에 저장한다.    \n",
    "<br/>\n",
    "예시: ``x.copy_(y)``, ``x.t_()``  \n",
    "      --> ``x``가 바뀔 것이다.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>In-place 연산을 사용해서 메모리를 절약할 수 있다. 그러나, 계산했던 기록이 즉각적으로 없어지기 때문에 미분을 계산할 때 문제가 될 수 있다. 그러므로, in-place 연산은 권장하지 않는다.</p></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Bridge with NumPy\n",
    "~~~~~~~~~~~~~~~~~\n",
    "Tensors와 NumPy arrays는 메모리 위치를 공유한다.\n",
    "그래서 tensor를 바꾸면, NumPy array도 바뀐다.\n",
    "역으로 NumPy array를 바꾸면 tensor도 바뀐다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor to NumPy array\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(\"t: {0}\".format(t))\n",
    "n = t.numpy()\n",
    "print(\"n: {0}\".format(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tensor를 바꾸면 NumPy array도 바뀐다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: tensor([2., 2., 2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(\"t: {0}\".format(t))\n",
    "print(\"n: {0}\".format(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy array to Tensor\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy array를 바꾸면 tensor도 바뀐다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(\"t: {0}\".format(t))\n",
    "print(\"n: {0}\".format(n))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
